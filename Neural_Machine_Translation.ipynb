{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dba199f",
   "metadata": {},
   "source": [
    "### Attention Based Neural translation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "391758db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from nmt_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0efa696d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 16695.93it/s]\n"
     ]
    }
   ],
   "source": [
    "#loading the dataset\n",
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59057b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n",
       " ('10.11.19', '2019-11-10'),\n",
       " ('9/10/70', '1970-09-10'),\n",
       " ('saturday april 28 1990', '1990-04-28'),\n",
       " ('thursday january 26 1995', '1995-01-26'),\n",
       " ('monday march 7 1983', '1983-03-07'),\n",
       " ('sunday may 22 1988', '1988-05-22'),\n",
       " ('08 jul 2008', '2008-07-08'),\n",
       " ('8 sep 1999', '1999-09-08'),\n",
       " ('thursday january 1 1981', '1981-01-01')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exploring data\n",
    "\n",
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79af08ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "#maximum length for input and output dates, in order to make all input data in the same length\n",
    "Tx = 30\n",
    "Ty = 10  # xxxx-xx-xx output will be in this format so all outputs will be in 10 character long\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "# Each character has its index X and Y we represent data as a list of indices. Then, we change each indices to \n",
    "# one hot encoding vector in depth axis \n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "111ad801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: monday march 7 1983\n",
      "Target date: 1983-03-07\n",
      "\n",
      "\n",
      "\n",
      "Source after preprocessing (indices):\n",
      " [24 26 25 16 13 34  0 24 13 28 15 20  0 10  0  4 12 11  6 36 36 36 36 36\n",
      " 36 36 36 36 36 36]\n",
      "\n",
      "Target after preprocessing (indices):\n",
      " [ 2 10  9  4  0  1  4  0  1  8]\n",
      "\n",
      "\n",
      "\n",
      "Source after preprocessing (one-hot):\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "\n",
      "Target after preprocessing (one-hot):\n",
      " [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Sum of exploration and understanding\n",
    "\n",
    "index = 5\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print('\\n')\n",
    "print(\"\\nSource after preprocessing (indices):\\n\", X[index])\n",
    "print(\"\\nTarget after preprocessing (indices):\\n\", Y[index])\n",
    "print('\\n')\n",
    "print(\"\\nSource after preprocessing (one-hot):\\n\", Xoh[index])\n",
    "print(\"\\nTarget after preprocessing (one-hot):\\n\", Yoh[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f3a3c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=1):\n",
    "    \"\"\"Softmax activation function.\n",
    "    # Arguments\n",
    "        x : Tensor.\n",
    "        axis: Integer, axis along which the softmax normalization is applied.\n",
    "    # Returns\n",
    "        Tensor, output of softmax transformation.\n",
    "    # Raises\n",
    "        ValueError: In case `dim(x) == 1`.\n",
    "    \"\"\"\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a2a9121",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_human_vocab = 37\n",
    "len_machine_vocab = 11\n",
    "\n",
    "n_s = 64 # number of units for the post-attention LSTM's hidden state \"s\"\n",
    "n_a = 32 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b155038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class neural_translation_model(layers.Layer):\n",
    "    \n",
    "    def __init__(self, Tx = 30, Ty = 10, n_a = 32, n_s = 64, human_vocab_size = len_human_vocab\n",
    "                                                           , machine_vocab_size = len_machine_vocab):\n",
    "        \n",
    "        # Default parameter for model\n",
    "        self.Tx = Tx\n",
    "        self.Ty = Ty        \n",
    "        self.n_a = n_a # number of units for the pre-attention, bi-directional LSTM's hidden state 'a' \n",
    "        self.n_s = n_s # number of units for the post-attention LSTM's hidden state \"s\"\n",
    "        self.human_vocab_size = human_vocab_size\n",
    "        self.machine_vocab_size = machine_vocab_size\n",
    "        \n",
    "        \n",
    "        \n",
    "        # We will share weights with those layer. In order to prevent them to be intialized for each time step we can either \n",
    "        # define them as a global variable or we can create their object\n",
    "        self.repeator = layers.RepeatVector(Tx)\n",
    "        self.concatenator =  layers.Concatenate(axis=-1)\n",
    "        self.densor1 = layers.Dense(10, activation = \"tanh\")\n",
    "        self.densor2 = layers.Dense(1, activation = \"relu\")\n",
    "        self.activator = layers.Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "        self.dotor = layers.Dot(axes = 1)\n",
    "        \n",
    "        self.post_activation_LSTM_cell = layers.LSTM(n_s, return_state = True) # Please do not modify this global variable.\n",
    "        self.output_layer = layers.Dense(len(machine_vocab), activation=softmax)\n",
    "        \n",
    "    def a_step_attention(self, a, s_prev):\n",
    "        #it is same activation that will be shared for all t_delta activations to calculate alpha\n",
    "        s_prev = self.repeator(s_prev)\n",
    "        #concatenate the activations with hidden state of post attention LSTM \n",
    "        concatenation = self.concatenator([a,s_prev])\n",
    "        \n",
    "        #Here is the small fully connected neural network to find attention weights \n",
    "        # intermediate energies\n",
    "        e = self.densor1(concatenation)\n",
    "        # Energies\n",
    "        energies = self.densor2(e)\n",
    "        #softmax to calculate alphas\n",
    "        alpha = self.activator(energies)\n",
    "        \n",
    "        # context = sum_over_t_x( alpha(t_y,t_x)) * a(t_x)\n",
    "        context = self.dotor([alpha,a])\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def model(self):\n",
    "        \n",
    "        X  = layers.Input(shape = (self.Tx,self.human_vocab_size))\n",
    "        s0 = layers.Input(shape = (self.n_s,), name ='s0')\n",
    "        c0 = layers.Input(shape = (self.n_s,), name ='c0')\n",
    "        \n",
    "        s = s0 \n",
    "        c = c0 \n",
    "        \n",
    "        a = layers.Bidirectional(layers.LSTM(self.n_a ,return_sequences= True))(X)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(self.Ty):\n",
    "            \n",
    "            context = self.a_step_attention(a, s)\n",
    "            \n",
    "            s, _, c = self.post_activation_LSTM_cell(context,initial_state=[s, c])\n",
    "            \n",
    "            out = self.output_layer(s)\n",
    "            \n",
    "            outputs.append(out)\n",
    "            \n",
    "        model = tf.keras.Model(inputs = [X,s0,c0] , outputs = outputs)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f359e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_model = neural_translation_model().model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "250d47aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 30, 37)]     0           []                               \n",
      "                                                                                                  \n",
      " s0 (InputLayer)                [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 30, 64)      17920       ['input_5[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " repeat_vector_4 (RepeatVector)  (None, 30, 64)      0           ['s0[0][0]',                     \n",
      "                                                                  'lstm_5[0][0]',                 \n",
      "                                                                  'lstm_5[1][0]',                 \n",
      "                                                                  'lstm_5[2][0]',                 \n",
      "                                                                  'lstm_5[3][0]',                 \n",
      "                                                                  'lstm_5[4][0]',                 \n",
      "                                                                  'lstm_5[5][0]',                 \n",
      "                                                                  'lstm_5[6][0]',                 \n",
      "                                                                  'lstm_5[7][0]',                 \n",
      "                                                                  'lstm_5[8][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 30, 128)      0           ['bidirectional_1[0][0]',        \n",
      "                                                                  'repeat_vector_4[0][0]',        \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'repeat_vector_4[1][0]',        \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'repeat_vector_4[2][0]',        \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'repeat_vector_4[3][0]',        \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'repeat_vector_4[4][0]',        \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'repeat_vector_4[5][0]',        \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'repeat_vector_4[6][0]',        \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'repeat_vector_4[7][0]',        \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'repeat_vector_4[8][0]',        \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'repeat_vector_4[9][0]']        \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 30, 10)       1290        ['concatenate_4[0][0]',          \n",
      "                                                                  'concatenate_4[1][0]',          \n",
      "                                                                  'concatenate_4[2][0]',          \n",
      "                                                                  'concatenate_4[3][0]',          \n",
      "                                                                  'concatenate_4[4][0]',          \n",
      "                                                                  'concatenate_4[5][0]',          \n",
      "                                                                  'concatenate_4[6][0]',          \n",
      "                                                                  'concatenate_4[7][0]',          \n",
      "                                                                  'concatenate_4[8][0]',          \n",
      "                                                                  'concatenate_4[9][0]']          \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 30, 1)        11          ['dense_12[0][0]',               \n",
      "                                                                  'dense_12[1][0]',               \n",
      "                                                                  'dense_12[2][0]',               \n",
      "                                                                  'dense_12[3][0]',               \n",
      "                                                                  'dense_12[4][0]',               \n",
      "                                                                  'dense_12[5][0]',               \n",
      "                                                                  'dense_12[6][0]',               \n",
      "                                                                  'dense_12[7][0]',               \n",
      "                                                                  'dense_12[8][0]',               \n",
      "                                                                  'dense_12[9][0]']               \n",
      "                                                                                                  \n",
      " attention_weights (Activation)  (None, 30, 1)       0           ['dense_13[0][0]',               \n",
      "                                                                  'dense_13[1][0]',               \n",
      "                                                                  'dense_13[2][0]',               \n",
      "                                                                  'dense_13[3][0]',               \n",
      "                                                                  'dense_13[4][0]',               \n",
      "                                                                  'dense_13[5][0]',               \n",
      "                                                                  'dense_13[6][0]',               \n",
      "                                                                  'dense_13[7][0]',               \n",
      "                                                                  'dense_13[8][0]',               \n",
      "                                                                  'dense_13[9][0]']               \n",
      "                                                                                                  \n",
      " dot_4 (Dot)                    (None, 1, 64)        0           ['attention_weights[0][0]',      \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'attention_weights[1][0]',      \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'attention_weights[2][0]',      \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'attention_weights[3][0]',      \n",
      "                                                                  'bidirectional_1[0][0]',        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  'attention_weights[4][0]',      \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'attention_weights[5][0]',      \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'attention_weights[6][0]',      \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'attention_weights[7][0]',      \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'attention_weights[8][0]',      \n",
      "                                                                  'bidirectional_1[0][0]',        \n",
      "                                                                  'attention_weights[9][0]',      \n",
      "                                                                  'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " c0 (InputLayer)                [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  [(None, 64),         33024       ['dot_4[0][0]',                  \n",
      "                                 (None, 64),                      's0[0][0]',                     \n",
      "                                 (None, 64)]                      'c0[0][0]',                     \n",
      "                                                                  'dot_4[1][0]',                  \n",
      "                                                                  'lstm_5[0][0]',                 \n",
      "                                                                  'lstm_5[0][2]',                 \n",
      "                                                                  'dot_4[2][0]',                  \n",
      "                                                                  'lstm_5[1][0]',                 \n",
      "                                                                  'lstm_5[1][2]',                 \n",
      "                                                                  'dot_4[3][0]',                  \n",
      "                                                                  'lstm_5[2][0]',                 \n",
      "                                                                  'lstm_5[2][2]',                 \n",
      "                                                                  'dot_4[4][0]',                  \n",
      "                                                                  'lstm_5[3][0]',                 \n",
      "                                                                  'lstm_5[3][2]',                 \n",
      "                                                                  'dot_4[5][0]',                  \n",
      "                                                                  'lstm_5[4][0]',                 \n",
      "                                                                  'lstm_5[4][2]',                 \n",
      "                                                                  'dot_4[6][0]',                  \n",
      "                                                                  'lstm_5[5][0]',                 \n",
      "                                                                  'lstm_5[5][2]',                 \n",
      "                                                                  'dot_4[7][0]',                  \n",
      "                                                                  'lstm_5[6][0]',                 \n",
      "                                                                  'lstm_5[6][2]',                 \n",
      "                                                                  'dot_4[8][0]',                  \n",
      "                                                                  'lstm_5[7][0]',                 \n",
      "                                                                  'lstm_5[7][2]',                 \n",
      "                                                                  'dot_4[9][0]',                  \n",
      "                                                                  'lstm_5[8][0]',                 \n",
      "                                                                  'lstm_5[8][2]']                 \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 11)           715         ['lstm_5[0][0]',                 \n",
      "                                                                  'lstm_5[1][0]',                 \n",
      "                                                                  'lstm_5[2][0]',                 \n",
      "                                                                  'lstm_5[3][0]',                 \n",
      "                                                                  'lstm_5[4][0]',                 \n",
      "                                                                  'lstm_5[5][0]',                 \n",
      "                                                                  'lstm_5[6][0]',                 \n",
      "                                                                  'lstm_5[7][0]',                 \n",
      "                                                                  'lstm_5[8][0]',                 \n",
      "                                                                  'lstm_5[9][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "attention_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ff1e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.005,beta_1 = 0.9,beta_2 = 0.999,decay = 0.01) \n",
    "attention_model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e04e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9eefc1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "157/157 [==============================] - 8s 49ms/step - loss: 1.5572 - dense_14_loss: 0.0231 - dense_14_1_loss: 0.0101 - dense_14_2_loss: 0.1727 - dense_14_3_loss: 0.2382 - dense_14_4_loss: 0.0025 - dense_14_5_loss: 0.0524 - dense_14_6_loss: 0.3043 - dense_14_7_loss: 0.0029 - dense_14_8_loss: 0.3525 - dense_14_9_loss: 0.3986 - dense_14_accuracy: 0.9971 - dense_14_1_accuracy: 0.9986 - dense_14_2_accuracy: 0.9512 - dense_14_3_accuracy: 0.9462 - dense_14_4_accuracy: 1.0000 - dense_14_5_accuracy: 0.9849 - dense_14_6_accuracy: 0.9204 - dense_14_7_accuracy: 1.0000 - dense_14_8_accuracy: 0.8717 - dense_14_9_accuracy: 0.8766\n",
      "Epoch 2/15\n",
      "157/157 [==============================] - 9s 58ms/step - loss: 1.4027 - dense_14_loss: 0.0206 - dense_14_1_loss: 0.0085 - dense_14_2_loss: 0.1520 - dense_14_3_loss: 0.2075 - dense_14_4_loss: 0.0021 - dense_14_5_loss: 0.0498 - dense_14_6_loss: 0.2763 - dense_14_7_loss: 0.0029 - dense_14_8_loss: 0.3198 - dense_14_9_loss: 0.3632 - dense_14_accuracy: 0.9974 - dense_14_1_accuracy: 0.9989 - dense_14_2_accuracy: 0.9622 - dense_14_3_accuracy: 0.9592 - dense_14_4_accuracy: 1.0000 - dense_14_5_accuracy: 0.9854 - dense_14_6_accuracy: 0.9273 - dense_14_7_accuracy: 0.9999 - dense_14_8_accuracy: 0.8830 - dense_14_9_accuracy: 0.8876\n",
      "Epoch 3/15\n",
      "157/157 [==============================] - 11s 67ms/step - loss: 1.2714 - dense_14_loss: 0.0184 - dense_14_1_loss: 0.0072 - dense_14_2_loss: 0.1344 - dense_14_3_loss: 0.1808 - dense_14_4_loss: 0.0020 - dense_14_5_loss: 0.0468 - dense_14_6_loss: 0.2537 - dense_14_7_loss: 0.0026 - dense_14_8_loss: 0.2909 - dense_14_9_loss: 0.3348 - dense_14_accuracy: 0.9980 - dense_14_1_accuracy: 0.9994 - dense_14_2_accuracy: 0.9699 - dense_14_3_accuracy: 0.9714 - dense_14_4_accuracy: 1.0000 - dense_14_5_accuracy: 0.9868 - dense_14_6_accuracy: 0.9318 - dense_14_7_accuracy: 1.0000 - dense_14_8_accuracy: 0.8970 - dense_14_9_accuracy: 0.8945\n",
      "Epoch 4/15\n",
      "157/157 [==============================] - 8s 53ms/step - loss: 1.1649 - dense_14_loss: 0.0164 - dense_14_1_loss: 0.0062 - dense_14_2_loss: 0.1189 - dense_14_3_loss: 0.1579 - dense_14_4_loss: 0.0017 - dense_14_5_loss: 0.0447 - dense_14_6_loss: 0.2380 - dense_14_7_loss: 0.0024 - dense_14_8_loss: 0.2660 - dense_14_9_loss: 0.3127 - dense_14_accuracy: 0.9981 - dense_14_1_accuracy: 0.9995 - dense_14_2_accuracy: 0.9782 - dense_14_3_accuracy: 0.9797 - dense_14_4_accuracy: 1.0000 - dense_14_5_accuracy: 0.9864 - dense_14_6_accuracy: 0.9345 - dense_14_7_accuracy: 1.0000 - dense_14_8_accuracy: 0.9081 - dense_14_9_accuracy: 0.9010\n",
      "Epoch 5/15\n",
      "157/157 [==============================] - 9s 56ms/step - loss: 1.0729 - dense_14_loss: 0.0149 - dense_14_1_loss: 0.0055 - dense_14_2_loss: 0.1055 - dense_14_3_loss: 0.1412 - dense_14_4_loss: 0.0015 - dense_14_5_loss: 0.0433 - dense_14_6_loss: 0.2226 - dense_14_7_loss: 0.0023 - dense_14_8_loss: 0.2462 - dense_14_9_loss: 0.2898 - dense_14_accuracy: 0.9987 - dense_14_1_accuracy: 0.9995 - dense_14_2_accuracy: 0.9845 - dense_14_3_accuracy: 0.9849 - dense_14_4_accuracy: 1.0000 - dense_14_5_accuracy: 0.9863 - dense_14_6_accuracy: 0.9400 - dense_14_7_accuracy: 1.0000 - dense_14_8_accuracy: 0.9172 - dense_14_9_accuracy: 0.9087\n",
      "Epoch 6/15\n",
      "157/157 [==============================] - 8s 51ms/step - loss: 0.9936 - dense_14_loss: 0.0137 - dense_14_1_loss: 0.0049 - dense_14_2_loss: 0.0938 - dense_14_3_loss: 0.1255 - dense_14_4_loss: 0.0013 - dense_14_5_loss: 0.0416 - dense_14_6_loss: 0.2108 - dense_14_7_loss: 0.0022 - dense_14_8_loss: 0.2275 - dense_14_9_loss: 0.2722 - dense_14_accuracy: 0.9991 - dense_14_1_accuracy: 0.9996 - dense_14_2_accuracy: 0.9879 - dense_14_3_accuracy: 0.9891 - dense_14_4_accuracy: 1.0000 - dense_14_5_accuracy: 0.9882 - dense_14_6_accuracy: 0.9422 - dense_14_7_accuracy: 1.0000 - dense_14_8_accuracy: 0.9263 - dense_14_9_accuracy: 0.9124\n",
      "Epoch 7/15\n",
      "157/157 [==============================] - 8s 53ms/step - loss: 0.9261 - dense_14_loss: 0.0125 - dense_14_1_loss: 0.0044 - dense_14_2_loss: 0.0836 - dense_14_3_loss: 0.1140 - dense_14_4_loss: 0.0012 - dense_14_5_loss: 0.0397 - dense_14_6_loss: 0.2003 - dense_14_7_loss: 0.0021 - dense_14_8_loss: 0.2118 - dense_14_9_loss: 0.2565 - dense_14_accuracy: 0.9991 - dense_14_1_accuracy: 0.9996 - dense_14_2_accuracy: 0.9910 - dense_14_3_accuracy: 0.9914 - dense_14_4_accuracy: 1.0000 - dense_14_5_accuracy: 0.9883 - dense_14_6_accuracy: 0.9448 - dense_14_7_accuracy: 1.0000 - dense_14_8_accuracy: 0.9328 - dense_14_9_accuracy: 0.9197\n",
      "Epoch 8/15\n",
      "157/157 [==============================] - 8s 53ms/step - loss: 0.8640 - dense_14_loss: 0.0118 - dense_14_1_loss: 0.0041 - dense_14_2_loss: 0.0749 - dense_14_3_loss: 0.1026 - dense_14_4_loss: 0.0011 - dense_14_5_loss: 0.0389 - dense_14_6_loss: 0.1909 - dense_14_7_loss: 0.0020 - dense_14_8_loss: 0.1954 - dense_14_9_loss: 0.2423 - dense_14_accuracy: 0.9991 - dense_14_1_accuracy: 0.9996 - dense_14_2_accuracy: 0.9942 - dense_14_3_accuracy: 0.9935 - dense_14_4_accuracy: 1.0000 - dense_14_5_accuracy: 0.9883 - dense_14_6_accuracy: 0.9463 - dense_14_7_accuracy: 1.0000 - dense_14_8_accuracy: 0.9408 - dense_14_9_accuracy: 0.9234\n",
      "Epoch 9/15\n",
      "157/157 [==============================] - 8s 50ms/step - loss: 0.8069 - dense_14_loss: 0.0110 - dense_14_1_loss: 0.0037 - dense_14_2_loss: 0.0672 - dense_14_3_loss: 0.0945 - dense_14_4_loss: 9.5011e-04 - dense_14_5_loss: 0.0375 - dense_14_6_loss: 0.1822 - dense_14_7_loss: 0.0020 - dense_14_8_loss: 0.1818 - dense_14_9_loss: 0.2259 - dense_14_accuracy: 0.9992 - dense_14_1_accuracy: 0.9998 - dense_14_2_accuracy: 0.9958 - dense_14_3_accuracy: 0.9953 - dense_14_4_accuracy: 1.0000 - dense_14_5_accuracy: 0.9891 - dense_14_6_accuracy: 0.9490 - dense_14_7_accuracy: 1.0000 - dense_14_8_accuracy: 0.9476 - dense_14_9_accuracy: 0.9302\n",
      "Epoch 10/15\n",
      "157/157 [==============================] - 8s 51ms/step - loss: 0.7607 - dense_14_loss: 0.0103 - dense_14_1_loss: 0.0035 - dense_14_2_loss: 0.0607 - dense_14_3_loss: 0.0869 - dense_14_4_loss: 9.5471e-04 - dense_14_5_loss: 0.0366 - dense_14_6_loss: 0.1748 - dense_14_7_loss: 0.0020 - dense_14_8_loss: 0.1708 - dense_14_9_loss: 0.2142 - dense_14_accuracy: 0.9994 - dense_14_1_accuracy: 0.9997 - dense_14_2_accuracy: 0.9969 - dense_14_3_accuracy: 0.9954 - dense_14_4_accuracy: 1.0000 - dense_14_5_accuracy: 0.9884 - dense_14_6_accuracy: 0.9507 - dense_14_7_accuracy: 0.9999 - dense_14_8_accuracy: 0.9531 - dense_14_9_accuracy: 0.9335\n",
      "Epoch 11/15\n",
      "157/157 [==============================] - 10s 64ms/step - loss: 0.7214 - dense_14_loss: 0.0097 - dense_14_1_loss: 0.0032 - dense_14_2_loss: 0.0551 - dense_14_3_loss: 0.0803 - dense_14_4_loss: 8.9951e-04 - dense_14_5_loss: 0.0356 - dense_14_6_loss: 0.1692 - dense_14_7_loss: 0.0018 - dense_14_8_loss: 0.1620 - dense_14_9_loss: 0.2036 - dense_14_accuracy: 0.9994 - dense_14_1_accuracy: 0.9998 - dense_14_2_accuracy: 0.9974 - dense_14_3_accuracy: 0.9960 - dense_14_4_accuracy: 1.0000 - dense_14_5_accuracy: 0.9890 - dense_14_6_accuracy: 0.9500 - dense_14_7_accuracy: 1.0000 - dense_14_8_accuracy: 0.9546 - dense_14_9_accuracy: 0.9365\n",
      "Epoch 12/15\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 0.6797 - dense_14_loss: 0.0092 - dense_14_1_loss: 0.0030 - dense_14_2_loss: 0.0499 - dense_14_3_loss: 0.0753 - dense_14_4_loss: 8.0232e-04 - dense_14_5_loss: 0.0347 - dense_14_6_loss: 0.1619 - dense_14_7_loss: 0.0017 - dense_14_8_loss: 0.1515 - dense_14_9_loss: 0.1917 - dense_14_accuracy: 0.9996 - dense_14_1_accuracy: 0.9997 - dense_14_2_accuracy: 0.9980 - dense_14_3_accuracy: 0.9963 - dense_14_4_accuracy: 1.0000 - dense_14_5_accuracy: 0.9892 - dense_14_6_accuracy: 0.9525 - dense_14_7_accuracy: 1.0000 - dense_14_8_accuracy: 0.9581 - dense_14_9_accuracy: 0.9392\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 8s 53ms/step - loss: 0.6446 - dense_14_loss: 0.0087 - dense_14_1_loss: 0.0029 - dense_14_2_loss: 0.0458 - dense_14_3_loss: 0.0697 - dense_14_4_loss: 7.5778e-04 - dense_14_5_loss: 0.0335 - dense_14_6_loss: 0.1563 - dense_14_7_loss: 0.0017 - dense_14_8_loss: 0.1433 - dense_14_9_loss: 0.1821 - dense_14_accuracy: 0.9996 - dense_14_1_accuracy: 0.9998 - dense_14_2_accuracy: 0.9984 - dense_14_3_accuracy: 0.9965 - dense_14_4_accuracy: 1.0000 - dense_14_5_accuracy: 0.9891 - dense_14_6_accuracy: 0.9536 - dense_14_7_accuracy: 1.0000 - dense_14_8_accuracy: 0.9620 - dense_14_9_accuracy: 0.9433\n",
      "Epoch 14/15\n",
      "157/157 [==============================] - 8s 52ms/step - loss: 0.6113 - dense_14_loss: 0.0083 - dense_14_1_loss: 0.0027 - dense_14_2_loss: 0.0419 - dense_14_3_loss: 0.0663 - dense_14_4_loss: 6.9024e-04 - dense_14_5_loss: 0.0323 - dense_14_6_loss: 0.1504 - dense_14_7_loss: 0.0016 - dense_14_8_loss: 0.1349 - dense_14_9_loss: 0.1721 - dense_14_accuracy: 0.9996 - dense_14_1_accuracy: 0.9998 - dense_14_2_accuracy: 0.9983 - dense_14_3_accuracy: 0.9966 - dense_14_4_accuracy: 1.0000 - dense_14_5_accuracy: 0.9889 - dense_14_6_accuracy: 0.9556 - dense_14_7_accuracy: 1.0000 - dense_14_8_accuracy: 0.9666 - dense_14_9_accuracy: 0.9489\n",
      "Epoch 15/15\n",
      "157/157 [==============================] - 9s 55ms/step - loss: 0.5841 - dense_14_loss: 0.0079 - dense_14_1_loss: 0.0026 - dense_14_2_loss: 0.0383 - dense_14_3_loss: 0.0622 - dense_14_4_loss: 6.7426e-04 - dense_14_5_loss: 0.0314 - dense_14_6_loss: 0.1453 - dense_14_7_loss: 0.0016 - dense_14_8_loss: 0.1298 - dense_14_9_loss: 0.1644 - dense_14_accuracy: 0.9996 - dense_14_1_accuracy: 0.9998 - dense_14_2_accuracy: 0.9988 - dense_14_3_accuracy: 0.9968 - dense_14_4_accuracy: 1.0000 - dense_14_5_accuracy: 0.9900 - dense_14_6_accuracy: 0.9569 - dense_14_7_accuracy: 1.0000 - dense_14_8_accuracy: 0.9667 - dense_14_9_accuracy: 0.9516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c9efbb6310>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_model.fit([Xoh, s0, c0], outputs, epochs=15, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84ceab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "source: 3 May 1979\n",
      "output: 1979-05-03 \n",
      "\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "source: 5 April 09\n",
      "output: 2099-04-04 \n",
      "\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "source: 21 of August 2016\n",
      "output: 2016-08-01 \n",
      "\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10 \n",
      "\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09 \n",
      "\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "source: March 3 2001\n",
      "output: 2001-03-03 \n",
      "\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-30 \n",
      "\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21 of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "s00 = np.zeros((1, n_s))\n",
    "c00 = np.zeros((1, n_s))\n",
    "for example in EXAMPLES:\n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "    source = np.swapaxes(source, 0, 1)\n",
    "    source = np.expand_dims(source, axis=0)\n",
    "    prediction = attention_model.predict([source, s00, c00])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e55414",
   "metadata": {},
   "source": [
    "Results are pretty good for such a small training time. We can see that model succesfully extracts months and gets correspending month value for them. It also seeable that when we identify date with st,nd,rd (1st,2nd,3rd) it messes up partially. We could fix it with further training forward and adding more similar labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73a668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
